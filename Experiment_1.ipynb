{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import re\n",
    "#import nltk\n",
    "\n",
    "#stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\"[0-9]\", \" \", string)\n",
    "    string = string.replace(\"'\",\"\")\n",
    "    string = string.replace(\"`\",\"\")\n",
    "    \n",
    "    return string.strip()\n",
    "\n",
    "class Loader():\n",
    "\n",
    "    def read_WikipediaEventsDataset(self, vector_source, start, end, minConf):\n",
    "        listEvents = []\n",
    "        data_dir='data/WE'\n",
    "        for year in range(start,end+1):\n",
    "            filename='wiki-events-' + str(year) + '_data.json.gz'\n",
    "            print(\"loading file \" + filename + \" ...\")\n",
    "            with gzip.open(os.path.join(data_dir, filename), \"rb\") as f:\n",
    "                events = json.loads(f.read().decode(\"utf8\"))\n",
    "            print(\"found \" + str(len(events['results'])) + \" events...\")\n",
    "            listEvents = listEvents + events['results']\n",
    "            print(\"total: \" + str(len(listEvents)) + \" events...\")\n",
    "        \n",
    "        classes = ['armed conflicts and attacks', 'politics and elections', \n",
    "           'law and crime', 'disasters and accidents', 'international relations', \n",
    "           'sport', 'business and economy', 'arts and culture', 'science and technology']\n",
    "        \n",
    "        data = {}\n",
    "        x, y = [], []\n",
    "\n",
    "        for event in listEvents:\n",
    "            #check is the keys are present\n",
    "            if 'event-type' in event and 'full-text' in event:\n",
    "                #keep only events with non empty full-text and event-type\n",
    "                if event['full-text'] and event['event-type']:\n",
    "                    label = event['event-type']\n",
    "                    if (label in classes):\n",
    "                        entities = event['entities']\n",
    "                        if vector_source=='entities':\n",
    "                            vector = []\n",
    "                            for entity in entities:\n",
    "                                avgconf = sum(float(i) for i in entity['confidence'])/len(entity['confidence'])\n",
    "                                if avgconf>minConf:\n",
    "                                    vector.append(entity['label'])\n",
    "                        elif vector_source=='short-text':\n",
    "                            vector = event['event'].split()\n",
    "                        elif vector_source=='full-text':\n",
    "                            vector = clean_str(event['full-text']).split()\n",
    "                            #for i,x in vector:\n",
    "                                #vector[i] = stemmer.stem(x)\n",
    "                        y.append(label)\n",
    "                        x.append(vector)\n",
    "        x, y = shuffle(x, y)\n",
    "\n",
    "        print(\"number of selected events: \" + str(len(x)))\n",
    "\n",
    "        dev_idx = len(x) // 10 * 8\n",
    "        test_idx = len(x) // 10 * 9\n",
    "\n",
    "        data[\"train_x\"], data[\"train_y\"] = x[:dev_idx], y[:dev_idx]\n",
    "        data[\"dev_x\"], data[\"dev_y\"] = x[dev_idx:test_idx], y[dev_idx:test_idx]\n",
    "        data[\"test_x\"], data[\"test_y\"] = x[test_idx:], y[test_idx:]\n",
    "\n",
    "        print(\"# train: \" + str(len(data[\"train_x\"])))\n",
    "        print(\"# dev: \" + str(len(data[\"dev_x\"])))\n",
    "        print(\"# test: \" + str(len(data[\"test_x\"])))\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def read_TREC(self):\n",
    "        print(\"Reading TREC dataset...\")\n",
    "        data = {}\n",
    "\n",
    "        def read(mode):\n",
    "            x, y = [], []\n",
    "\n",
    "            with open(\"data/TREC/TREC_\" + mode + \".txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    if line[-1] == \"\\n\":\n",
    "                        line = line[:-1]\n",
    "                    y.append(line.split()[0].split(\":\")[0])\n",
    "                    x.append(line.split()[1:])\n",
    "\n",
    "            x, y = shuffle(x, y)\n",
    "\n",
    "            if mode == \"train\":\n",
    "                dev_idx = len(x) // 10\n",
    "                data[\"dev_x\"], data[\"dev_y\"] = x[:dev_idx], y[:dev_idx]\n",
    "                data[\"train_x\"], data[\"train_y\"] = x[dev_idx:], y[dev_idx:]\n",
    "            else:\n",
    "                data[\"test_x\"], data[\"test_y\"] = x, y\n",
    "\n",
    "        read(\"train\")\n",
    "        read(\"test\")\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def read_MR(self):\n",
    "        data = {}\n",
    "        x, y = [], []\n",
    "\n",
    "        with open(\"data/MR/rt-polarity.pos\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line[-1] == \"\\n\":\n",
    "                    line = line[:-1]\n",
    "                x.append(line.split())\n",
    "                y.append(1)\n",
    "\n",
    "        with open(\"data/MR/rt-polarity.neg\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line[-1] == \"\\n\":\n",
    "                    line = line[:-1]\n",
    "                x.append(line.split())\n",
    "                y.append(0)\n",
    "\n",
    "        x, y = shuffle(x, y)\n",
    "        dev_idx = len(x) // 10 * 8\n",
    "        test_idx = len(x) // 10 * 9\n",
    "\n",
    "        data[\"train_x\"], data[\"train_y\"] = x[:dev_idx], y[:dev_idx]\n",
    "        data[\"dev_x\"], data[\"dev_y\"] = x[dev_idx:test_idx], y[dev_idx:test_idx]\n",
    "        data[\"test_x\"], data[\"test_y\"] = x[test_idx:], y[test_idx:]\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def save_model(self, model, params):\n",
    "        path = f\"saved_models/{params['DATASET']}_{params['MODEL']}_{params['EPOCH']}_{params['LEARNING_RATE']}_{params['VECTORS_FROM']}.pkl\"\n",
    "        pickle.dump(model, open(path, \"wb\"))\n",
    "        print(f\"A model is saved successfully as {path}!\")\n",
    "\n",
    "\n",
    "    def load_model(self, params):\n",
    "        path = f\"saved_models/{params['DATASET']}_{params['MODEL']}_{params['EPOCH']}_{params['LEARNING_RATE']}_{params['VECTORS_FROM']}.pkl\"\n",
    "\n",
    "        try:\n",
    "            model = pickle.load(open(path, \"rb\"))\n",
    "            print(f\"Model in {path} loaded successfully!\")\n",
    "\n",
    "            return model\n",
    "        except:\n",
    "            print(f\"No available model such as {path}.\")\n",
    "            exit()\n",
    "\n",
    "myloader = Loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file wiki-events-2012_data.json.gz ...\n",
      "found 5745 events...\n",
      "total: 5745 events...\n",
      "loading file wiki-events-2013_data.json.gz ...\n",
      "found 4951 events...\n",
      "total: 10696 events...\n",
      "number of selected events: 8417\n",
      "# train: 6728\n",
      "# dev: 841\n",
      "# test: 848\n",
      "CPU times: user 17.6 s, sys: 2.87 s, total: 20.5 s\n",
      "Wall time: 20.6 s\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "%time data = myloader.read_WikipediaEventsDataset('short-text',2012,2013,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[\"vocab\"] = sorted(list(set([w for sent in data[\"train_x\"] + data[\"dev_x\"] + data[\"test_x\"] for w in sent])))\n",
    "data[\"classes\"] = sorted(list(set(data[\"train_y\"])))\n",
    "data[\"word_to_idx\"] = {w: i for i, w in enumerate(data[\"vocab\"])}\n",
    "data[\"idx_to_word\"] = {i: w for i, w in enumerate(data[\"vocab\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set global parmas and load data\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "sys.argv = ['-h']\n",
    "parser = argparse.ArgumentParser(description=\"-----[CNN-classifier]-----\")\n",
    "parser.add_argument(\"--mode\", default=\"train\", help=\"train: train (with test) a model / test: test saved models\")\n",
    "parser.add_argument(\"--architecture\", default=\"CNN\", help=\"available achitectures: CBOW, CNN\")\n",
    "parser.add_argument(\"--vectors-from\", default=\"short-text\", help=\"available: full-text, short-text\")\n",
    "parser.add_argument(\"--model\", default=\"rand\", help=\"available models: rand, static, non-static, multichannel\")\n",
    "parser.add_argument(\"--dataset\", default=\"WE\", help=\"available datasets: MR, TREC, WE\")\n",
    "parser.add_argument(\"--save_model\", default=False, action='store_true', help=\"whether saving model or not\")\n",
    "parser.add_argument(\"--early_stopping\", default=False, action='store_true', help=\"whether to apply early stopping\")\n",
    "parser.add_argument(\"--epoch\", default=1, type=int, help=\"number of max epoch\")\n",
    "parser.add_argument(\"--learning_rate\", default=1.0, type=float, help=\"learning rate\")\n",
    "parser.add_argument(\"--gpu\", default=0, type=int, help=\"the number of gpu to be used\")\n",
    "\n",
    "options = parser.parse_args()\n",
    "#data = getattr(myloader, f\"read_{options.dataset}\")()\n",
    "\n",
    "params = {\n",
    "    \"MODEL\": 'non-static',#options.model,\n",
    "    \"ARCHITECTURE\": 'CBOW',#options.architecture,\n",
    "    \"DATASET\": \"WE\",#options.dataset,\n",
    "    \"EMBEDDINGS_FILE\": \"embeddings/GoogleNews-vectors-negative300.bin\",\n",
    "    \"VECTORS_FROM\": \"short-text\",\n",
    "    \"SAVE_MODEL\": True, #options.save_model,\n",
    "    \"EARLY_STOPPING\": options.early_stopping,\n",
    "    \"EPOCH\": 1, #options.epoch,\n",
    "    \"LEARNING_RATE\": 0.1, #options.learning_rate,\n",
    "    \"MAX_SENT_LEN\": max([len(sent) for sent in data[\"train_x\"] + data[\"dev_x\"] + data[\"test_x\"]]),\n",
    "    \"BATCH_SIZE\": 50,\n",
    "    \"WORD_DIM\": 300,\n",
    "    \"VOCAB_SIZE\": len(data[\"vocab\"]),\n",
    "    \"CLASS_SIZE\": len(data[\"classes\"]),\n",
    "    \"FILTERS\": [3, 4, 5],\n",
    "    \"FILTER_NUM\": [100, 100, 100],\n",
    "    \"DROPOUT_PROB\": 0.1,\n",
    "    \"NORM_LIMIT\": 3,\n",
    "    \"GPU\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#look at data samples\n",
    "#data['train_x'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x153a077050>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.MODEL = kwargs[\"MODEL\"]\n",
    "        self.BATCH_SIZE = kwargs[\"BATCH_SIZE\"]\n",
    "        self.MAX_SENT_LEN = kwargs[\"MAX_SENT_LEN\"]\n",
    "        self.WORD_DIM = kwargs[\"WORD_DIM\"]\n",
    "        self.VOCAB_SIZE = kwargs[\"VOCAB_SIZE\"]\n",
    "        self.CLASS_SIZE = kwargs[\"CLASS_SIZE\"]\n",
    "        self.FILTERS = kwargs[\"FILTERS\"]\n",
    "        self.FILTER_NUM = kwargs[\"FILTER_NUM\"]\n",
    "        self.DROPOUT_PROB = kwargs[\"DROPOUT_PROB\"]\n",
    "        self.IN_CHANNEL = 1\n",
    "\n",
    "        assert (len(self.FILTERS) == len(self.FILTER_NUM))\n",
    "\n",
    "        # one for UNK and one for zero padding\n",
    "        self.embedding = nn.Embedding(self.VOCAB_SIZE + 2, self.WORD_DIM, padding_idx=self.VOCAB_SIZE + 1)\n",
    "        if self.MODEL == \"static\" or self.MODEL == \"non-static\" or self.MODEL == \"multichannel\":\n",
    "            self.WV_MATRIX = kwargs[\"WV_MATRIX\"]\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(self.WV_MATRIX))\n",
    "            if self.MODEL == \"static\":\n",
    "                self.embedding.weight.requires_grad = False\n",
    "            elif self.MODEL == \"multichannel\":\n",
    "                self.embedding2 = nn.Embedding(self.VOCAB_SIZE + 2, self.WORD_DIM, padding_idx=self.VOCAB_SIZE + 1)\n",
    "                self.embedding2.weight.data.copy_(torch.from_numpy(self.WV_MATRIX))\n",
    "                self.embedding2.weight.requires_grad = False\n",
    "                self.IN_CHANNEL = 2\n",
    "\n",
    "        for i in range(len(self.FILTERS)):\n",
    "            conv = nn.Conv1d(self.IN_CHANNEL, self.FILTER_NUM[i], self.WORD_DIM * self.FILTERS[i], stride=self.WORD_DIM)\n",
    "            setattr(self, f'conv_{i}', conv)\n",
    "\n",
    "        self.fc = nn.Linear(sum(self.FILTER_NUM), self.CLASS_SIZE)\n",
    "\n",
    "    def get_conv(self, i):\n",
    "        return getattr(self, f'conv_{i}')\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = self.embedding(inp).view(-1, 1, self.WORD_DIM * self.MAX_SENT_LEN)\n",
    "        if self.MODEL == \"multichannel\":\n",
    "            x2 = self.embedding2(inp).view(-1, 1, self.WORD_DIM * self.MAX_SENT_LEN)\n",
    "            x = torch.cat((x, x2), 1)\n",
    "\n",
    "        conv_results = [\n",
    "            F.max_pool1d(F.relu(self.get_conv(i)(x)), self.MAX_SENT_LEN - self.FILTERS[i] + 1)\n",
    "                .view(-1, self.FILTER_NUM[i])\n",
    "            for i in range(len(self.FILTERS))]\n",
    "\n",
    "        x = torch.cat(conv_results, 1)\n",
    "        x = F.dropout(x, p=self.DROPOUT_PROB, training=self.training)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CBOW, self).__init__()\n",
    "        \n",
    "        self.MODEL = kwargs[\"MODEL\"]\n",
    "        self.BATCH_SIZE = kwargs[\"BATCH_SIZE\"]\n",
    "        self.WORD_DIM = kwargs[\"WORD_DIM\"]\n",
    "        self.VOCAB_SIZE = kwargs[\"VOCAB_SIZE\"]\n",
    "        self.CLASS_SIZE = kwargs[\"CLASS_SIZE\"]\n",
    "        self.DROPOUT_PROB = kwargs[\"DROPOUT_PROB\"]\n",
    "        \n",
    "        self.embeddings = nn.Embedding(self.VOCAB_SIZE + 2, self.WORD_DIM, padding_idx=self.VOCAB_SIZE + 1)\n",
    "        self.WV_MATRIX = kwargs[\"WV_MATRIX\"]\n",
    "        print(\"WV matrix size: \" + str(self.WV_MATRIX.shape))\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(self.WV_MATRIX))\n",
    "        if self.MODEL == \"static\":\n",
    "                self.embeddings.weight.requires_grad = False\n",
    "        self.linear1 = nn.Linear(self.WORD_DIM, 128)\n",
    "        self.linear2 = nn.Linear(128, self.CLASS_SIZE)     \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #embeds = terch.tensor(50,300)\n",
    "        #for inp in self.embeddings(inputs):\n",
    "        #    embeds.append(sum(in))\n",
    "        #embeds = sum(self.embeddings(inputs)).view((1, -1))\n",
    "        embeds = torch.sum(self.embeddings(inputs), dim=1)\n",
    "        #print(\"Inputs size: \" + str(self.embeddings(inputs).size()))\n",
    "        #print(\"Embeds size: \" + str(embeds.size()))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = F.dropout(out, p=self.DROPOUT_PROB, training=self.training)\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word2vec...\n",
      "CPU times: user 41.5 s, sys: 5.27 s, total: 46.8 s\n",
      "Wall time: 48.2 s\n"
     ]
    }
   ],
   "source": [
    "# load word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "print(\"loading word2vec...\")\n",
    "%time word_vectors = KeyedVectors.load_word2vec_format(params[\"EMBEDDINGS_FILE\"], binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE THE EXPERIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "#The Embeddings:\n",
    "wv_matrix = []\n",
    "\n",
    "def train(data, params):\n",
    "\n",
    "    wv_matrix = []\n",
    "    if params[\"MODEL\"] != \"rand\":\n",
    "        for i in range(len(data[\"vocab\"])):\n",
    "            word = data[\"idx_to_word\"][i]\n",
    "            if word in word_vectors.vocab:\n",
    "                wv_matrix.append(word_vectors.word_vec(word))\n",
    "            else:\n",
    "                wv_matrix.append(np.random.uniform(-0.01, 0.01, 300).astype(\"float32\"))\n",
    "\n",
    "        # one for UNK and one for zero padding\n",
    "        wv_matrix.append(np.random.uniform(-0.01, 0.01, 300).astype(\"float32\"))\n",
    "        wv_matrix.append(np.zeros(300).astype(\"float32\"))\n",
    "        wv_matrix = np.array(wv_matrix)\n",
    "        params[\"WV_MATRIX\"] = wv_matrix\n",
    "  \n",
    "    if params[\"ARCHITECTURE\"] == \"CBOW\":\n",
    "        print(\"Initializing CBOW module...\")\n",
    "        model = CBOW(**params)\n",
    "    else:\n",
    "        print(\"Initializing CNN module...\")\n",
    "        #model = CNN(**params).cuda(params[\"GPU\"])\n",
    "        model = CNN(**params)\n",
    "    \n",
    "    #Force CPU usage to avoid probelms with parallel computation\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    #Parallel CPU\n",
    "    #torch.nn.DataParallel(model)\n",
    "    \n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = optim.Adadelta(parameters, params[\"LEARNING_RATE\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    pre_dev_acc = 0\n",
    "    max_dev_acc = 0\n",
    "    max_test_acc = 0\n",
    "    for e in range(params[\"EPOCH\"]):\n",
    "        data[\"train_x\"], data[\"train_y\"] = shuffle(data[\"train_x\"], data[\"train_y\"])\n",
    "\n",
    "        for i in range(0, len(data[\"train_x\"]), params[\"BATCH_SIZE\"]):\n",
    "            batch_range = min(params[\"BATCH_SIZE\"], len(data[\"train_x\"]) - i)\n",
    "\n",
    "            batch_x = [[data[\"word_to_idx\"][w] for w in sent] +\n",
    "                       [params[\"VOCAB_SIZE\"] + 1] * (params[\"MAX_SENT_LEN\"] - len(sent))\n",
    "                       for sent in data[\"train_x\"][i:i + batch_range]]\n",
    "            batch_y = [data[\"classes\"].index(c) for c in data[\"train_y\"][i:i + batch_range]]\n",
    "\n",
    "            #batch_x = Variable(torch.LongTensor(batch_x)).cuda(params[\"GPU\"])\n",
    "            #batch_y = Variable(torch.LongTensor(batch_y)).cuda(params[\"GPU\"])\n",
    "            batch_x = Variable(torch.LongTensor(batch_x))\n",
    "            batch_y = Variable(torch.LongTensor(batch_y))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model.train()\n",
    "            pred = model(batch_x)\n",
    "            loss = criterion(pred, batch_y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(parameters, max_norm=params[\"NORM_LIMIT\"])\n",
    "            optimizer.step()\n",
    "            if (i%1000==0):\n",
    "                print(str(i) + \" steps: Loss = \" + str(loss));\n",
    "            \n",
    "\n",
    "        dev_acc = test(data, model, params, mode=\"dev\")\n",
    "        test_acc = test(data, model, params)\n",
    "        print(\"epoch:\", e + 1, \"/ dev_acc:\", dev_acc, \"/ test_acc:\", test_acc)\n",
    "\n",
    "        if params[\"EARLY_STOPPING\"] and dev_acc <= pre_dev_acc:\n",
    "            print(\"early stopping by dev_acc!\")\n",
    "            break\n",
    "        else:\n",
    "            pre_dev_acc = dev_acc\n",
    "\n",
    "        if dev_acc > max_dev_acc:\n",
    "            max_dev_acc = dev_acc\n",
    "            max_test_acc = test_acc\n",
    "            #best_model = copy.deepcopy(model)\n",
    "            best_model = model\n",
    "\n",
    "    print(\"max dev acc:\", max_dev_acc, \"test acc:\", max_test_acc)\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def test(data, model, params, mode=\"test\"):\n",
    "    model.eval()\n",
    "\n",
    "    if mode == \"dev\":\n",
    "        x, y = data[\"dev_x\"], data[\"dev_y\"]\n",
    "    elif mode == \"test\":\n",
    "        x, y = data[\"test_x\"], data[\"test_y\"]\n",
    "\n",
    "    x = [[data[\"word_to_idx\"][w] if w in data[\"vocab\"] else params[\"VOCAB_SIZE\"] for w in sent] +\n",
    "         [params[\"VOCAB_SIZE\"] + 1] * (params[\"MAX_SENT_LEN\"] - len(sent))\n",
    "         for sent in x]\n",
    "\n",
    "    #x = Variable(torch.LongTensor(x)).cuda(params[\"GPU\"])\n",
    "    x = Variable(torch.LongTensor(x))\n",
    "\n",
    "    y = [data[\"classes\"].index(c) for c in y]\n",
    "\n",
    "    pred = np.argmax(model(x).cpu().data.numpy(), axis=1)\n",
    "    acc = sum([1 if p == y else 0 for p, y in zip(pred, y)]) / len(pred)\n",
    "\n",
    "    return acc\n",
    "\n",
    "def testSamples(data, model, params, start, end):\n",
    "    model.eval()\n",
    "    x, y = data[\"test_x\"][:start], data[\"test_y\"][:end]\n",
    "\n",
    "    x = [[data[\"word_to_idx\"][w] if w in data[\"vocab\"] else params[\"VOCAB_SIZE\"] for w in sent] +\n",
    "         [params[\"VOCAB_SIZE\"] + 1] * (params[\"MAX_SENT_LEN\"] - len(sent))\n",
    "         for sent in x]\n",
    "\n",
    "    #x = Variable(torch.LongTensor(x)).cuda(params[\"GPU\"])\n",
    "    x = Variable(torch.LongTensor(x))\n",
    "\n",
    "    y = [data[\"classes\"].index(c) for c in y]\n",
    "\n",
    "    pred = np.argmax(model(x).cpu().data.numpy(), axis=1)\n",
    "    for i in range(0,9):\n",
    "        print(\"\\n\" + str(data[\"test_x\"][i]) + \"\\nClass: \" + data[\"test_y\"][i])\n",
    "        print(\"PredictedClass: \" + str(data[\"classes\"][pred[i]])) \n",
    "    #acc = sum([1 if p == y else 0 for p, y in zip(pred, y)]) / len(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN THE EXPERIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n",
      "====================INFORMATION====================\n",
      "MODEL: non-static\n",
      "ARCHITECTURE: CBOW\n",
      "DATASET: WE\n",
      "VECTORS_FROM: short-text\n",
      "VOCAB_SIZE: 29591\n",
      "MAX_SENT_LEN: 110\n",
      "EPOCH: 1\n",
      "LEARNING_RATE: 0.1\n",
      "EARLY_STOPPING: False\n",
      "SAVE_MODEL: True\n",
      "====================INFORMATION====================\n",
      "====================TRAINING STARTED====================\n",
      "Initializing CBOW module...\n",
      "WV matrix size: (29593, 300)\n",
      "0 steps: Loss = tensor(2.1940)\n",
      "1000 steps: Loss = tensor(1.7423)\n",
      "2000 steps: Loss = tensor(1.5160)\n",
      "3000 steps: Loss = tensor(1.3547)\n",
      "4000 steps: Loss = tensor(1.0534)\n",
      "5000 steps: Loss = tensor(1.0919)\n",
      "6000 steps: Loss = tensor(1.2003)\n",
      "epoch: 1 / dev_acc: 0.7336504161712247 / test_acc: 0.6969339622641509\n",
      "max dev acc: 0.7336504161712247 test acc: 0.6969339622641509\n",
      "A model is saved successfully as saved_models/WE_non-static_1_0.1_short-text.pkl!\n",
      "====================TRAINING FINISHED====================\n"
     ]
    }
   ],
   "source": [
    "#Do the training and evaluation\n",
    "%time\n",
    "\n",
    "print(\"=\" * 20 + \"INFORMATION\" + \"=\" * 20)\n",
    "print(\"MODEL:\", params[\"MODEL\"])\n",
    "print(\"ARCHITECTURE:\", params[\"ARCHITECTURE\"])\n",
    "print(\"DATASET:\", params[\"DATASET\"])\n",
    "print(\"VECTORS_FROM:\", params[\"VECTORS_FROM\"])\n",
    "print(\"VOCAB_SIZE:\", params[\"VOCAB_SIZE\"])\n",
    "print(\"MAX_SENT_LEN:\", params[\"MAX_SENT_LEN\"])\n",
    "print(\"EPOCH:\", params[\"EPOCH\"])\n",
    "print(\"LEARNING_RATE:\", params[\"LEARNING_RATE\"])\n",
    "print(\"EARLY_STOPPING:\", params[\"EARLY_STOPPING\"])\n",
    "print(\"SAVE_MODEL:\", params[\"SAVE_MODEL\"])\n",
    "print(\"=\" * 20 + \"INFORMATION\" + \"=\" * 20)\n",
    "\n",
    "model = None\n",
    "\n",
    "if options.mode == \"train\":\n",
    "    print(\"=\" * 20 + \"TRAINING STARTED\" + \"=\" * 20)\n",
    "    model = train(data, params)\n",
    "    if params[\"SAVE_MODEL\"]:\n",
    "        myloader.save_model(model, params)\n",
    "    print(\"=\" * 20 + \"TRAINING FINISHED\" + \"=\" * 20)\n",
    "else:\n",
    "    #model = load_model(params).cuda(params[\"GPU\"])\n",
    "    model = myloader.load_model(params)\n",
    "\n",
    "    test_acc = test(data, model, params)\n",
    "    print(\"test acc:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['A', 'Ghanaian', 'national', 'goes', 'on', 'a', 'spree', 'attack', 'with', 'a', 'pickaxe', 'in', 'Milan,', 'Italy', 'killing', 'a', 'passerby', 'and', 'wounding', 'four', 'others', 'in', 'an', 'apparently', 'random', 'attack.', '(AP', 'via', 'News24)']\n",
      "Class: armed conflicts and attacks\n",
      "PredictedClass: armed conflicts and attacks\n",
      "\n",
      "['India', 'enacts', 'new', 'rules', 'designed', 'to', 'make', 'it', 'more', 'difficult', 'for', 'foreign', 'investors', 'to', 'use', 'the', 'country', 'as', 'a', 'tax', 'dodge.', '(Bloomberg)']\n",
      "Class: business and economy\n",
      "PredictedClass: business and economy\n",
      "\n",
      "['Barack', 'Obama', 'and', 'Raul', 'Castro', 'exchange', 'handshakes', 'despite', 'a', 'United', 'States', 'embargo', 'against', 'Cuba.', '(ABC', 'News)']\n",
      "Class: arts and culture\n",
      "PredictedClass: politics and elections\n",
      "\n",
      "['A', 'female', 'suicide', 'bomber', 'kills', 'four', 'and', 'injures', 'four', 'outside', 'a', 'hospital', 'in', 'Khaar,', 'Federally', 'Administered', 'Tribal', 'Areas,', 'Pakistan.', '(PTI', 'via', 'The', 'Hindu)']\n",
      "Class: armed conflicts and attacks\n",
      "PredictedClass: armed conflicts and attacks\n",
      "\n",
      "['One', 'person', 'is', 'killed', 'and', 'ten', 'are', 'injured', 'by', 'a', 'tornado', 'in', 'north', 'west', 'Poland', '(BBC)']\n",
      "Class: disasters and accidents\n",
      "PredictedClass: disasters and accidents\n",
      "\n",
      "['Insurgents', 'launch', 'an', 'attack', 'on', 'the', 'strategic', 'city', 'of', 'Gao,', 'capturing', 'a', 'police', 'station', 'and', 'sparking', 'a', 'day-long', 'firefight', 'that', 'included', 'Malian', 'troops,', 'as', 'well', 'as', 'French', 'attack', 'helicopters.', '(Al', 'Jazeera)']\n",
      "Class: armed conflicts and attacks\n",
      "PredictedClass: armed conflicts and attacks\n",
      "\n",
      "['Forbes', 'releases', 'its', 'annual', 'list', 'of', 'the', \"world's\", 'wealthiest', 'people,', 'with', 'Mexican', 'business', 'magnate', 'Carlos', 'Slim', 'topping', 'the', 'list', 'for', 'the', 'fourth', 'year', 'in', 'a', 'row.', '(USA', 'Today)']\n",
      "Class: business and economy\n",
      "PredictedClass: business and economy\n",
      "\n",
      "['Thein', 'Sein,', 'the', 'President', 'of', 'Burma', '(Myanmar)', 'visits', 'the', 'United', 'Kingdom.', '(The', 'Telegraph', 'via', 'the', 'Brisbane', 'Times)']\n",
      "Class: international relations\n",
      "PredictedClass: international relations\n",
      "\n",
      "['Indian', 'sitar', 'virtuoso', 'and', 'classical', 'composer', 'Ravi', 'Shankar', 'dies', 'in', 'the', 'U.S.', 'city', 'of', 'San', 'Diego', 'at', 'the', 'age', 'of', '92.', '(BBC)']\n",
      "Class: arts and culture\n",
      "PredictedClass: arts and culture\n"
     ]
    }
   ],
   "source": [
    "testSamples(data, model, params, 110 , 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4982\n",
      "VECTOR: 0\n"
     ]
    }
   ],
   "source": [
    "idx = data[\"word_to_idx\"]['Carnival']\n",
    "print(str(idx) + \"\\nVECTOR: \" + str(len(wv_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
